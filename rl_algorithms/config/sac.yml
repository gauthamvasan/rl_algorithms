# Learner
gamma: 0.99
alpha: 0.2
lr: 0.0003
batch_size: 256
n_epochs: 1
polyak: 0.995
n_buffer: 1000000
steps_between_updates: 1
n_warmup: 1000

# Two hidden layers with 64 units each, ReLU activations
mlp:
  hidden_sizes: [64, 64]
  activation: "relu"

# Task
env: "Reacher-v2"
